{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Multi-Image Attentional Pooling Test\n",
    "\n",
    "Testing the new multi-image attentional pooling functionality with dummy data to ensure no runtime errors."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/envs/cls/lib/python3.10/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "/opt/conda/envs/cls/lib/python3.10/site-packages/timm/models/layers/__init__.py:48: FutureWarning: Importing from timm.models.layers is deprecated, please import via timm.layers\n",
      "  warnings.warn(f\"Importing from {__name__} is deprecated, please import via timm.layers\", FutureWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PyTorch version: 2.5.1+cu121\n",
      "CUDA available: True\n",
      "CUDA device: NVIDIA A100-SXM4-80GB\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import numpy as np\n",
    "from epsclassifiers.intern_vit_classifier.intern_vit_classifier import InternVitClassifier\n",
    "\n",
    "print(f\"PyTorch version: {torch.__version__}\")\n",
    "print(f\"CUDA available: {torch.cuda.is_available()}\")\n",
    "if torch.cuda.is_available():\n",
    "    print(f\"CUDA device: {torch.cuda.get_device_name()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Configuration\n",
    "INTERN_VL_CHECKPOINT_DIR = \"/mnt/data/intervl_weights/no_labels/internvl3_chimera_20250609_233409_1e-5_epsilon_all_0608/checkpoint-24934\"\n",
    "INTERN_VIT_OUTPUT_DIM = 3200  # 3200 for InternVL 26B model\n",
    "NUM_CLASSES = 1\n",
    "BATCH_SIZE = 8  # Required due to BatchNorm1d limitation\n",
    "IMAGE_SIZE = 448  # Standard input size for InternVL\n",
    "NUM_CHANNELS = 3"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Test 1: Single Image with Attentional Pooling (Baseline)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The argument `trust_remote_code` is to be used with Auto classes. It has no effect here and is ignored.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== Test 1: Single Image with Attentional Pooling ===\n",
      "WARNING: Because of BatchNorm1d that doesn't work on single element batches, InternVitClassifier currently supports only batch sizes >= 2\n",
      "trainable params: 40,370,176 || all params: 7,653,191,168 || trainable%: 0.5275\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading checkpoint shards: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 6/6 [00:02<00:00,  2.41it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO: InternVitClassifier will NOT be using multi image input and will NOT be using tile splitting\n",
      "INFO: InternVitClassifier will be using attentive pooling\n",
      "Input shape: torch.Size([8, 3, 448, 448])\n",
      "Output keys: dict_keys(['output', 'embeddings', 'last_hidden_state', 'attention_weights'])\n",
      "Output shape: torch.Size([8, 1])\n",
      "Embeddings shape: torch.Size([8, 3200])\n",
      "Attention weights shape: torch.Size([8, 1, 1024])\n",
      "âœ… Single image test passed!\n",
      "ðŸ§¹ Memory cleared\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(\"=== Test 1: Single Image with Attentional Pooling ===\")\n",
    "\n",
    "# Create model for single image input\n",
    "model_single = InternVitClassifier(\n",
    "    num_classes=NUM_CLASSES,\n",
    "    intern_vl_checkpoint_dir=INTERN_VL_CHECKPOINT_DIR,\n",
    "    intern_vit_output_dim=INTERN_VIT_OUTPUT_DIM,\n",
    "    multi_image_input=False,\n",
    "    use_attentional_pooling=True,\n",
    "\n",
    ")\n",
    "\n",
    "model_single.eval().to('cuda').to(torch.bfloat16)\n",
    "\n",
    "# Create dummy single image data\n",
    "dummy_images = torch.randn(BATCH_SIZE, NUM_CHANNELS, IMAGE_SIZE, IMAGE_SIZE).to('cuda').to(torch.bfloat16)\n",
    "\n",
    "# Test forward pass\n",
    "with torch.no_grad():\n",
    "    output = model_single(dummy_images)\n",
    "\n",
    "print(f\"Input shape: {dummy_images.shape}\")\n",
    "print(f\"Output keys: {output.keys()}\")\n",
    "print(f\"Output shape: {output['output'].shape}\")\n",
    "print(f\"Embeddings shape: {output['embeddings'].shape}\")\n",
    "print(f\"Attention weights shape: {output['attention_weights'].shape}\")\n",
    "print(\"âœ… Single image test passed!\")\n",
    "\n",
    "# Clear memory\n",
    "del model_single, dummy_images, output\n",
    "torch.cuda.empty_cache()\n",
    "print(\"ðŸ§¹ Memory cleared\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Test 2: Multi-Image with Fixed Number of Images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The argument `trust_remote_code` is to be used with Auto classes. It has no effect here and is ignored.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== Test 2: Multi-Image with Fixed Number (2 images) ===\n",
      "WARNING: Because of BatchNorm1d that doesn't work on single element batches, InternVitClassifier currently supports only batch sizes >= 2\n",
      "trainable params: 40,370,176 || all params: 7,653,191,168 || trainable%: 0.5275\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading checkpoint shards: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 6/6 [00:01<00:00,  3.03it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO: InternVitClassifier will be using multi image input of size 2\n",
      "INFO: InternVitClassifier will be using attentive pooling\n",
      "Input shape: torch.Size([8, 2, 3, 448, 448])\n",
      "Output keys: dict_keys(['output', 'embeddings', 'last_hidden_state', 'attention_weights'])\n",
      "Output shape: torch.Size([8, 1])\n",
      "Embeddings shape: torch.Size([8, 3200])\n",
      "Attention weights shape: torch.Size([8, 1, 2048])\n",
      "âœ… Multi-image fixed test passed!\n",
      "ðŸ§¹ Memory cleared\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(\"=== Test 2: Multi-Image with Fixed Number (2 images) ===\")\n",
    "\n",
    "NUM_MULTI_IMAGES = 2\n",
    "\n",
    "# Create model for multi-image input with fixed number\n",
    "model_multi_fixed = InternVitClassifier(\n",
    "    num_classes=NUM_CLASSES,\n",
    "    intern_vl_checkpoint_dir=INTERN_VL_CHECKPOINT_DIR,\n",
    "    intern_vit_output_dim=INTERN_VIT_OUTPUT_DIM,\n",
    "    multi_image_input=True,\n",
    "    num_multi_images=NUM_MULTI_IMAGES,\n",
    "    use_attentional_pooling=True,\n",
    "\n",
    ")\n",
    "\n",
    "model_multi_fixed.eval().to('cuda').to(torch.bfloat16)\n",
    "\n",
    "# Create dummy multi-image data: (batch_size, num_images, channels, height, width)\n",
    "dummy_multi_images = torch.randn(BATCH_SIZE, NUM_MULTI_IMAGES, NUM_CHANNELS, IMAGE_SIZE, IMAGE_SIZE).to('cuda').to(torch.bfloat16)\n",
    "\n",
    "# Test forward pass\n",
    "with torch.no_grad():\n",
    "    output = model_multi_fixed(dummy_multi_images)\n",
    "\n",
    "print(f\"Input shape: {dummy_multi_images.shape}\")\n",
    "print(f\"Output keys: {output.keys()}\")\n",
    "print(f\"Output shape: {output['output'].shape}\")\n",
    "print(f\"Embeddings shape: {output['embeddings'].shape}\")\n",
    "print(f\"Attention weights shape: {output['attention_weights'].shape}\")\n",
    "print(\"âœ… Multi-image fixed test passed!\")\n",
    "\n",
    "# Clear memory\n",
    "del model_multi_fixed, dummy_multi_images, output\n",
    "torch.cuda.empty_cache()\n",
    "print(\"ðŸ§¹ Memory cleared\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Test 3: Multi-Image with Variable Number of Images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The argument `trust_remote_code` is to be used with Auto classes. It has no effect here and is ignored.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== Test 3: Multi-Image with Variable Number ===\n",
      "WARNING: Because of BatchNorm1d that doesn't work on single element batches, InternVitClassifier currently supports only batch sizes >= 2\n",
      "trainable params: 40,370,176 || all params: 7,653,191,168 || trainable%: 0.5275\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading checkpoint shards: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 6/6 [00:01<00:00,  3.31it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO: InternVitClassifier will be using multi image input of size None\n",
      "INFO: InternVitClassifier will be using attentive pooling\n",
      "Input batch 1 images: 2\n",
      "Input batch 2 images: 4\n",
      "Input image shapes: [torch.Size([3, 448, 448]), torch.Size([3, 448, 448])]\n",
      "Output keys: dict_keys(['output', 'embeddings', 'last_hidden_state', 'attention_weights'])\n",
      "Output shape: torch.Size([2, 1])\n",
      "Embeddings shape: torch.Size([2, 3200])\n",
      "Attention weights shape: torch.Size([2, 1, 4096])\n",
      "âœ… Multi-image variable test passed!\n",
      "ðŸ§¹ Memory cleared\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(\"=== Test 3: Multi-Image with Variable Number ===\")\n",
    "\n",
    "# Create model for multi-image input with variable number\n",
    "model_multi_var = InternVitClassifier(\n",
    "    num_classes=NUM_CLASSES,\n",
    "    intern_vl_checkpoint_dir=INTERN_VL_CHECKPOINT_DIR,\n",
    "    intern_vit_output_dim=INTERN_VIT_OUTPUT_DIM,\n",
    "    multi_image_input=True,\n",
    "    num_multi_images=None,  # Variable number\n",
    "    use_attentional_pooling=True,\n",
    ")\n",
    "\n",
    "model_multi_var.eval().to('cuda').to(torch.bfloat16)\n",
    "\n",
    "# Create dummy variable multi-image data as list of tensors\n",
    "# Batch 1: 2 images, Batch 2: 4 images\n",
    "batch_1_images = [torch.randn(NUM_CHANNELS, IMAGE_SIZE, IMAGE_SIZE).to('cuda').to(torch.bfloat16) for _ in range(2)]\n",
    "batch_2_images = [torch.randn(NUM_CHANNELS, IMAGE_SIZE, IMAGE_SIZE).to('cuda').to(torch.bfloat16) for _ in range(4)]\n",
    "dummy_var_images = [batch_1_images, batch_2_images]\n",
    "\n",
    "# Test forward pass\n",
    "with torch.no_grad():\n",
    "    output = model_multi_var(dummy_var_images)\n",
    "\n",
    "print(f\"Input batch 1 images: {len(dummy_var_images[0])}\")\n",
    "print(f\"Input batch 2 images: {len(dummy_var_images[1])}\")\n",
    "print(f\"Input image shapes: {[img.shape for img in dummy_var_images[0]]}\")\n",
    "print(f\"Output keys: {output.keys()}\")\n",
    "print(f\"Output shape: {output['output'].shape}\")\n",
    "print(f\"Embeddings shape: {output['embeddings'].shape}\")\n",
    "print(f\"Attention weights shape: {output['attention_weights'].shape}\")\n",
    "print(\"âœ… Multi-image variable test passed!\")\n",
    "\n",
    "# Clear memory\n",
    "del model_multi_var, batch_1_images, batch_2_images, dummy_var_images, output\n",
    "torch.cuda.empty_cache()\n",
    "print(\"ðŸ§¹ Memory cleared\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Test 4: Multi-Image without Attentional Pooling (Comparison)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The argument `trust_remote_code` is to be used with Auto classes. It has no effect here and is ignored.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== Test 4: Multi-Image without Attentional Pooling ===\n",
      "WARNING: Because of BatchNorm1d that doesn't work on single element batches, InternVitClassifier currently supports only batch sizes >= 2\n",
      "trainable params: 40,370,176 || all params: 7,653,191,168 || trainable%: 0.5275\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading checkpoint shards: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 6/6 [00:01<00:00,  3.64it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO: InternVitClassifier will be using multi image input of size 2\n",
      "Input shape: torch.Size([8, 2, 3, 448, 448])\n",
      "Output keys: dict_keys(['output', 'embeddings', 'last_hidden_state', 'attention_weights'])\n",
      "Output shape: torch.Size([8, 1])\n",
      "Embeddings shape: torch.Size([8, 6400])\n",
      "Attention weights: None\n",
      "âœ… Multi-image without attentional pooling test passed!\n",
      "ðŸ§¹ Memory cleared\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(\"=== Test 4: Multi-Image without Attentional Pooling ===\")\n",
    "\n",
    "# Create model for multi-image input without attentional pooling\n",
    "model_multi_no_attn = InternVitClassifier(\n",
    "    num_classes=NUM_CLASSES,\n",
    "    intern_vl_checkpoint_dir=INTERN_VL_CHECKPOINT_DIR,\n",
    "    intern_vit_output_dim=INTERN_VIT_OUTPUT_DIM,\n",
    "    multi_image_input=True,\n",
    "    num_multi_images=2,  # Use fixed number for this test\n",
    "    use_attentional_pooling=False,  # No attentional pooling\n",
    ")\n",
    "\n",
    "model_multi_no_attn.eval().to('cuda').to(torch.bfloat16)\n",
    "\n",
    "# Create dummy multi-image data for this test\n",
    "dummy_multi_images_no_attn = torch.randn(BATCH_SIZE, 2, NUM_CHANNELS, IMAGE_SIZE, IMAGE_SIZE).to('cuda').to(torch.bfloat16)\n",
    "\n",
    "# Test forward pass\n",
    "with torch.no_grad():\n",
    "    output = model_multi_no_attn(dummy_multi_images_no_attn)\n",
    "\n",
    "print(f\"Input shape: {dummy_multi_images_no_attn.shape}\")\n",
    "print(f\"Output keys: {output.keys()}\")\n",
    "print(f\"Output shape: {output['output'].shape}\")\n",
    "print(f\"Embeddings shape: {output['embeddings'].shape}\")\n",
    "print(f\"Attention weights: {output['attention_weights']}\")\n",
    "print(\"âœ… Multi-image without attentional pooling test passed!\")\n",
    "\n",
    "# Clear memory\n",
    "del model_multi_no_attn, dummy_multi_images_no_attn, output\n",
    "torch.cuda.empty_cache()\n",
    "print(\"ðŸ§¹ Memory cleared\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Test Summary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== Test Summary ===\n",
      "âœ… All tests completed successfully!\n",
      "âœ… Single image attentional pooling works\n",
      "âœ… Multi-image with fixed number works\n",
      "âœ… Multi-image with variable number works\n",
      "âœ… Multi-image without attentional pooling works (comparison)\n",
      "\n",
      "ðŸŽ‰ Multi-image attentional pooling implementation is ready!\n"
     ]
    }
   ],
   "source": [
    "print(\"=== Test Summary ===\")\n",
    "print(\"âœ… All tests completed successfully!\")\n",
    "print(\"âœ… Single image attentional pooling works\")\n",
    "print(\"âœ… Multi-image with fixed number works\")\n",
    "print(\"âœ… Multi-image with variable number works\")\n",
    "print(\"âœ… Multi-image without attentional pooling works (comparison)\")\n",
    "print(\"\\nðŸŽ‰ Multi-image attentional pooling implementation is ready!\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "cls",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
